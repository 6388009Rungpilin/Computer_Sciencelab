{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Environment"
      ],
      "metadata": {
        "id": "534bUJbSFJFP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "adS6Iv30Dxdi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math, copy\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font = {'size' : 18}\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "np.set_printoptions(precision=3)  # reduced display precision on numpy arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "Suppose you are starting a company that grows and sells wild mushrooms. \n",
        "- Since not all mushrooms are edible, you'd like to be able to tell whether a given mushroom is edible or poisonous based on it's physical attributes\n",
        "- You have some existing data that you can use for this task. \n",
        "\n",
        "Can you use the data to help you identify which mushrooms can be sold safely?\n",
        "\n",
        "Suppose that you have 10 examples of mushrooms as follows:\n",
        "\n",
        "| Cap Color | Stalk Shape | Solitary | Edible |\n",
        "|:---------:|:-----------:|:--------:|:------:|\n",
        "|   Brown   |   Tapering  |    Yes   |    1   |\n",
        "|   Brown   |  Enlarging  |    Yes   |    1   |\n",
        "|   Brown   |  Enlarging  |    No    |    0   |\n",
        "|   Brown   |  Enlarging  |    No    |    0   |\n",
        "|   Brown   |   Tapering  |    Yes   |    1   |\n",
        "|    Red    |   Tapering  |    Yes   |    0   |\n",
        "|    Red    |  Enlarging  |    No    |    0   |\n",
        "|   Brown   |  Enlarging  |    Yes   |    1   |\n",
        "|    Red    |   Tapering  |    No    |    1   |\n",
        "|   Brown   |  Enlarging  |    No    |    0   |\n",
        "\n",
        "For each example, you have\n",
        "    - Three features\n",
        "        - Cap Color (`Brown` or `Red`),\n",
        "        - Stalk Shape (`Tapering` or `Enlarging`), and\n",
        "        - Solitary (`Yes` or `No`)\n",
        "    - Label\n",
        "        - Edible (`1` indicating yes or `0` indicating poisonous)\n",
        "\n",
        "Note: The dataset used is for illustrative purposes only. It is not meant to be a guide on identifying edible mushrooms.\n",
        "\n",
        "Source: The dataset used in this lab exercise is partly from Coursera - [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)"
      ],
      "metadata": {
        "id": "OZTFHHxNaiYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([\n",
        "    ['Brown','Tapering','Yes'],\n",
        "    ['Brown','Enlarging','Yes'],\n",
        "    ['Brown','Enlarging','No'],\n",
        "    ['Brown','Enlarging','No'],\n",
        "    ['Brown','Tapering','Yes'],\n",
        "    ['Red','Tapering','Yes'],\n",
        "    ['Red','Enlarging','No'],\n",
        "    ['Brown','Enlarging','Yes'],\n",
        "    ['Red','Tapering','No'],\n",
        "    ['Brown','Enlarging','No']\n",
        "])\n",
        "y = np.array([1,1,0,0,1,0,0,1,1,0])\n",
        "feature_names = ['Cap Color','Stalk Shape','Solitary']\n",
        "class_names = ['Poisonous', 'Edible']\n",
        "\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mOS7UO6i5iq",
        "outputId": "cc4b1996-cfd7-4f4b-cb88-d5fc9f71208f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 3) (10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot Encoding\n",
        "\n",
        "As you can see from the table above, all of the feature columns are still not numerical. Please transform such column into numerical ones using [`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). \n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
        "\n",
        "list(le.classes_)\n",
        "# Output: ['amsterdam', 'paris', 'tokyo']\n",
        "\n",
        "le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
        "# Output: [2 2 1]\n",
        "\n",
        "list(le.inverse_transform([2, 2, 1]))\n",
        "# Output: ['tokyo', 'tokyo', 'paris']\n",
        "```"
      ],
      "metadata": {
        "id": "64AJaUiEbzYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoders = []\n",
        "X_oh = np.zeros_like(X, dtype=int)\n",
        "\n",
        "for c in range(X.shape[1]):\n",
        "    # TODO: Create a label encoder\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # TODO: Fit the label encoder to each feature column\n",
        "    le.fit(X[:,c])\n",
        "\n",
        "    # TODO: Transform categorical to one-hot using the fitted label encoder\n",
        "    X_oh[:,c] = le.transform(X[:,c])\n",
        "\n",
        "    # TODO: Save the fitted label encoder to the list for prediction\n",
        "    label_encoders.append(le)\n",
        "\n",
        "print(X_oh)\n",
        "for fn, le in zip(feature_names, label_encoders):\n",
        "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    print(f\"{fn}: {le_name_mapping}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx-L7sMWZhTM",
        "outputId": "55d8502c-e42d-449f-df63-118ebffaab4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1]\n",
            " [0 0 1]\n",
            " [0 0 0]\n",
            " [0 0 0]\n",
            " [0 1 1]\n",
            " [1 1 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 1 0]\n",
            " [0 0 0]]\n",
            "Cap Color: {'Brown': 0, 'Red': 1}\n",
            "Stalk Shape: {'Enlarging': 0, 'Tapering': 1}\n",
            "Solitary: {'No': 0, 'Yes': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "```\n",
        "[[0 1 1]\n",
        " [0 0 1]\n",
        " [0 0 0]\n",
        " [0 0 0]\n",
        " [0 1 1]\n",
        " [1 1 1]\n",
        " [1 0 0]\n",
        " [0 0 1]\n",
        " [1 1 0]\n",
        " [0 0 0]]\n",
        "Cap Color: {'Brown': 0, 'Red': 1}\n",
        "Stalk Shape: {'Enlarging': 0, 'Tapering': 1}\n",
        "Solitary: {'No': 0, 'Yes': 1}\n",
        "```"
      ],
      "metadata": {
        "id": "6lHvbXD7koC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Learning\n",
        "\n",
        "The steps for building a decision tree are as follows:\n",
        "\n",
        "1. Start with all examples at the root node\n",
        "1. Calculate information gain for splitting on all possible features, and pick the one with the highest information gain\n",
        "1. Split dataset according to the selected feature, and create left and right branches of the tree\n",
        "1. Keep repeating splitting process until stopping criteria is met"
      ],
      "metadata": {
        "id": "-8Uz9KL_pYJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Entropy as Measure of Impurity\n",
        "\n",
        "To calculate the information gain, we first need to implement a function to calculate the entropy, `compute_entropy`, at a node.\n",
        "* This function takes in a numpy array (y) that indicates whether the examples in that node are edible (`1`) or poisonous (`0`).\n",
        "\n",
        "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n",
        "* Note \n",
        "    * The log is calculated with base $2$\n",
        "    * For implementation purposes, $0\\text{log}_2(0) = 0$. That is, if `p_1 = 0` or `p_1 = 1`, set the entropy to `0`\n",
        "    * Make sure to check that the data at a node is not empty (i.e. `len(y) != 0`). Return `0` if it is.\n",
        "    * log2 can be computed using [`np.log2`](https://numpy.org/doc/stable/reference/generated/numpy.log2.html)."
      ],
      "metadata": {
        "id": "ofJKShvkmKZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_entropy(y):\n",
        "    \"\"\"\n",
        "    Computes the entropy for \n",
        "    \n",
        "    Args:\n",
        "       y (ndarray): Numpy array indicating whether each example at a node is\n",
        "           edible (`1`) or poisonous (`0`)\n",
        "       \n",
        "    Returns:\n",
        "        entropy (float): Entropy at that node\n",
        "        \n",
        "    \"\"\"\n",
        "    # You need to return the following variables correctly\n",
        "    entropy = 0.\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Hint: For p1 = 0 and 1, set the entropy to 0 (to handle 0log0)\n",
        "    if len(y) != 0:\n",
        "        p1 = len(y[y == 1]) / len(y) \n",
        "        if p1 != 0 and p1 != 1:\n",
        "             entropy = -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return entropy"
      ],
      "metadata": {
        "id": "TxxnHOPyZhQo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the function."
      ],
      "metadata": {
        "id": "8AUPCB-moh3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute entropy at the root node (i.e. with all examples)\n",
        "print(\"Entropy at root node: \", compute_entropy(np.array([0,0,0,1,0,1,1,0,1,1]))) \n",
        "\n",
        "_y = np.array([1] * 10)\n",
        "result = compute_entropy(_y)\n",
        "print(result)\n",
        "\n",
        "assert result == 0, \"Entropy must be 0 with array of ones\"\n",
        "\n",
        "_y = np.array([0] * 10)\n",
        "result = compute_entropy(_y)\n",
        "print(result)\n",
        "\n",
        "assert result == 0, \"Entropy must be 0 with array of zeros\"\n",
        "\n",
        "_y = np.array([0] * 12 + [1] * 12)\n",
        "result = compute_entropy(_y)\n",
        "print(result)\n",
        "\n",
        "assert result == 1, \"Entropy must be 1 with same ammount of ones and zeros\"\n",
        "\n",
        "_y = np.array([1, 0, 1, 0, 1, 1, 1, 0, 1])\n",
        "assert np.isclose(compute_entropy(_y), 0.918295, atol=1e-6), \"Wrong value. Something between 0 and 1\"\n",
        "assert np.isclose(compute_entropy(-_y + 1), compute_entropy(_y), atol=1e-6), \"Wrong value\"\n",
        "print('All test passed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnp49GAZogsz",
        "outputId": "640d32ab-eac7-498b-fb44-2b50b599835d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy at root node:  1.0\n",
            "0.0\n",
            "0.0\n",
            "1.0\n",
            "All test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected outputs:**\n",
        "```\n",
        "Entropy at root node:  1.0\n",
        "0.0\n",
        "0.0\n",
        "1.0\n",
        "All test passed\n",
        "```"
      ],
      "metadata": {
        "id": "LIYh6gGVo7Rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: In this lab exercise, the entropy refers to the [binary entropy](https://en.wikipedia.org/wiki/Binary_entropy_function) as the possible values of the target are binary. The entropy equation for more than two classes is beyond the scope of this exercise and can be found [here](https://www.kaggle.com/code/prashant111/decision-tree-classifier-tutorial?scriptVersionId=30115184&cellId=11)."
      ],
      "metadata": {
        "id": "DqC0jVB4nT6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Dataset\n",
        "\n",
        "A helper function called `split_dataset` that takes in the data at a node and a feature to split on and splits it into left and right branches. Later in the lab, you'll implement code to calculate how good the split is.\n",
        "\n",
        "- The function takes in the training data, the list of indices of data points at that node, along with the feature to split on. \n",
        "- It splits the data and returns the subset of indices at the left and the right branch.\n",
        "- For example, say we're starting at the root node (so `node_indices = [0,1,2,3,4,5,6,7,8,9]`), and we chose to split on feature `0`, which is whether or not the example has a brown cap.\n",
        "    - The output of the function is then, `left_indices = [0,1,2,3,4,7,9]` and `right_indices = [5,6,8]`\n",
        "    \n",
        "| Index | Brown Cap | Tapering Stalk Shape | Solitary | Edible |\n",
        "|:-----:|:---------:|:--------------------:|:--------:|:------:|\n",
        "|   0   |     1     |           1          |     1    |    1   |\n",
        "|   1   |     1     |           0          |     1    |    1   |\n",
        "|   2   |     1     |           0          |     0    |    0   |\n",
        "|   3   |     1     |           0          |     0    |    0   |\n",
        "|   4   |     1     |           1          |     1    |    1   |\n",
        "|   5   |     0     |           1          |     1    |    0   |\n",
        "|   6   |     0     |           0          |     0    |    0   |\n",
        "|   7   |     1     |           0          |     1    |    1   |\n",
        "|   8   |     0     |           1          |     0    |    1   |\n",
        "|   9   |     1     |           0          |     0    |    0   |"
      ],
      "metadata": {
        "id": "eAsAebAmpuI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(X, node_indices, feature):\n",
        "    \"\"\"\n",
        "    Splits the data at the given node into\n",
        "    left and right branches\n",
        "    \n",
        "    Args:\n",
        "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
        "        node_indices (list):  List containing the active indices. I.e, the samples being considered at this step.\n",
        "        feature (int):           Index of feature to split on\n",
        "    \n",
        "    Returns:\n",
        "        left_indices (list): Indices with feature value == 1\n",
        "        right_indices (list): Indices with feature value == 0\n",
        "    \"\"\"\n",
        "    \n",
        "    # You need to return the following variables correctly\n",
        "    left_indices = []\n",
        "    right_indices = []\n",
        "    \n",
        "    for i in node_indices:   \n",
        "        if X[i][feature] == 1:\n",
        "            left_indices.append(i)\n",
        "        else:\n",
        "            right_indices.append(i)\n",
        "        \n",
        "    return left_indices, right_indices"
      ],
      "metadata": {
        "id": "wSZtm3_gZhOM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the example."
      ],
      "metadata": {
        "id": "X0unWk2xp2AP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "# Feel free to play around with these variables\n",
        "# The dataset only has three features, so this value can be 0 (Brown Cap), 1 (Tapering Stalk Shape) or 2 (Solitary)\n",
        "feature = 0\n",
        "\n",
        "left_indices, right_indices = split_dataset(X_oh, root_indices, feature)\n",
        "\n",
        "print(\"Left indices: \", left_indices)\n",
        "print(\"Right indices: \", right_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGQdQNIUp1eZ",
        "outputId": "d15d8e56-3352-473e-ae9b-912662296ba8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left indices:  [5, 6, 8]\n",
            "Right indices:  [0, 1, 2, 3, 4, 7, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Information Gain\n",
        "\n",
        "Next, you'll write a function called `information_gain` that takes in the training data, the indices at a node and a feature to split on and returns the information gain from the split.\n",
        "\n",
        "The `compute_information_gain()` function shown below to compute\n",
        "\n",
        "$$\\text{Information Gain} = H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))$$\n",
        "\n",
        "where \n",
        "- $H(p_1^\\text{node})$ is entropy at the node \n",
        "- $H(p_1^\\text{left})$ and $H(p_1^\\text{right})$ are the entropies at the left and the right branches resulting from the split\n",
        "- $w^{\\text{left}}$ and $w^{\\text{right}}$ are the proportion of examples at the left and right branch, respectively\n",
        "\n",
        "Note:\n",
        "- You can use the `compute_entropy()` function that you implemented above to calculate the entropy\n",
        "- We've provided some starter code that uses the `split_dataset()` function you implemented above to split the dataset \n",
        "\n",
        "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
      ],
      "metadata": {
        "id": "KqvNd_t1paKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_information_gain(X, y, node_indices, feature):\n",
        "    \n",
        "    \"\"\"\n",
        "    Compute the information of splitting the node on a given feature\n",
        "    \n",
        "    Args:\n",
        "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
        "        y (array like):         list or ndarray with n_samples containing the target variable\n",
        "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
        "   \n",
        "    Returns:\n",
        "        cost (float):        Cost computed\n",
        "    \n",
        "    \"\"\"    \n",
        "    # Split dataset\n",
        "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
        "    \n",
        "    # Some useful variables\n",
        "    X_node, y_node = X[node_indices], y[node_indices]\n",
        "    X_left, y_left = X[left_indices], y[left_indices]\n",
        "    X_right, y_right = X[right_indices], y[right_indices]\n",
        "    \n",
        "    # You need to return the following variables correctly\n",
        "    information_gain = 0\n",
        "    \n",
        "    # TODO: Use `compute_entropy` to calculate the entropy for each term\n",
        "    # node_entropy = # YOUR CODE HERE\n",
        "    # left_entropy = # YOUR CODE HERE\n",
        "    # right_entropy = # YOUR CODE HERE\n",
        "    node_entropy = compute_entropy(y_node)\n",
        "    left_entropy = compute_entropy(y_left)\n",
        "    right_entropy = compute_entropy(y_right)\n",
        "    \n",
        "    # TODO: Calculate weights for each branch (left and right)\n",
        "    # w_left = # YOUR CODE HERE\n",
        "    # w_right = # YOUR CODE HERE\n",
        "    w_left = len(X_left) / len(X_node)\n",
        "    w_right = len(X_right) / len(X_node)\n",
        "    \n",
        "    # TODO: Calculate information gain\n",
        "    # information_gain = # YOUR CODE HERE\n",
        "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
        "    information_gain = node_entropy - weighted_entropy\n",
        "    \n",
        "    return information_gain"
      ],
      "metadata": {
        "id": "NOibTUiXZhLl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the function."
      ],
      "metadata": {
        "id": "1XGl1RoVrWib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info_gain0 = compute_information_gain(X_oh, y, root_indices, feature=0)\n",
        "print(\"Information Gain from splitting the root feature 0: \", info_gain0)\n",
        "    \n",
        "info_gain1 = compute_information_gain(X_oh, y, root_indices, feature=1)\n",
        "print(\"Information Gain from splitting the root on feature 1: \", info_gain1)\n",
        "\n",
        "info_gain2 = compute_information_gain(X_oh, y, root_indices, feature=2)\n",
        "print(\"Information Gain from splitting the root on feature 2: \", info_gain2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh76ug-lZhJD",
        "outputId": "514a0b74-98b8-4dca-fb84-5664e7e864af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information Gain from splitting the root feature 0:  0.034851554559677034\n",
            "Information Gain from splitting the root on feature 1:  0.12451124978365313\n",
            "Information Gain from splitting the root on feature 2:  0.2780719051126377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected outputs:**\n",
        "```\n",
        "Information Gain from splitting the root feature 0:  0.034851554559677034\n",
        "Information Gain from splitting the root on feature 1:  0.12451124978365313\n",
        "Information Gain from splitting the root on feature 2:  0.2780719051126377\n",
        "```"
      ],
      "metadata": {
        "id": "W7G29gCCs0bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: According to the information gain we computed earlier, which question should we ask at the root node?\n",
        "\n",
        "Explain your answer below with the supporting evidence."
      ],
      "metadata": {
        "id": "M3aMeWuks8wU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "Possible Answer: The best question to ask at the root node is \"whether the mushroom is solitary or not?\". This is because splitting the root on the solitary yields the highest information gain."
      ],
      "metadata": {
        "id": "HxBXpaQGtbu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the DecisionTree using scikit-learn\n",
        "\n",
        "Once we understand the concept and have implemented some of the code to determine the information gain, which is an important measure of impurity, we are now ready to use the existing library to build a decision tree.\n",
        "\n",
        "The following is an example of how to build a [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) model in scikit-learn.\n",
        "\n",
        "```\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "```\n",
        "\n",
        "By default, the [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) will use `gini` is a measure of impurity. Please refer to the scikit-learn API and learn how to create a decision tree that will use entropy as the impurity measure, and create the tree below."
      ],
      "metadata": {
        "id": "38qFtomjxL3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create a tree that use `entropy` as a impurity measure\n",
        "# model = # YOUR CODE HERE\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier(criterion='entropy')"
      ],
      "metadata": {
        "id": "CPdMUvj7ZhCO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's build a decision tree classifier from the mushroom dataset."
      ],
      "metadata": {
        "id": "NIZ06RKmzAD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "model.fit(X_oh, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_bFLzPLy_fm",
        "outputId": "59855b48-ae6d-49f7-dc67-2c902eccc845"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(criterion='entropy')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, use the built tree to make predictions on the dataset."
      ],
      "metadata": {
        "id": "4XrfDmK8zUNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y_preds = # YOUR CODE HERE\n",
        "y_preds = model.predict(X_oh)"
      ],
      "metadata": {
        "id": "jm6TIGrbZg_N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute the model performance in terms of accuracy."
      ],
      "metadata": {
        "id": "7wjdEZHH57gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {np.mean(y_preds == y)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9kt2PNsZg8_",
        "outputId": "e5d28eab-7e4f-4e14-bb8c-492653b53774"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Accuracy**: 1.0"
      ],
      "metadata": {
        "id": "h5puf0cwzr-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visulize the decision tree."
      ],
      "metadata": {
        "id": "cuXZo4vG4Cl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "from sklearn import tree\n",
        "\n",
        "dot_data = tree.export_graphviz(\n",
        "    model, out_file=None, \n",
        "    feature_names=feature_names,\n",
        "    class_names=class_names,\n",
        "    filled=True, rounded=True,\n",
        "    special_characters=True)\n",
        "\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "0yTrguBMZg60",
        "outputId": "9b7f4386-318d-4d08-8d35-3e90b429e083"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.files.Source at 0x7f76b3d152e0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"526pt\" height=\"314pt\"\n viewBox=\"0.00 0.00 526.00 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-310 522,-310 522,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M309,-306C309,-306 207,-306 207,-306 201,-306 195,-300 195,-294 195,-294 195,-235 195,-235 195,-229 201,-223 207,-223 207,-223 309,-223 309,-223 315,-223 321,-229 321,-235 321,-235 321,-294 321,-294 321,-300 315,-306 309,-306\"/>\n<text text-anchor=\"start\" x=\"218.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Solitary ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"218\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.0</text>\n<text text-anchor=\"start\" x=\"217\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10</text>\n<text text-anchor=\"start\" x=\"218.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5, 5]</text>\n<text text-anchor=\"start\" x=\"203\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Poisonous</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#eca06a\" stroke=\"black\" d=\"M241,-187C241,-187 139,-187 139,-187 133,-187 127,-181 127,-175 127,-175 127,-116 127,-116 127,-110 133,-104 139,-104 139,-104 241,-104 241,-104 247,-104 253,-110 253,-116 253,-116 253,-175 253,-175 253,-181 247,-187 241,-187\"/>\n<text text-anchor=\"start\" x=\"136.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Stalk Shape ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"142.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.722</text>\n<text text-anchor=\"start\" x=\"152.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\n<text text-anchor=\"start\" x=\"150.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4, 1]</text>\n<text text-anchor=\"start\" x=\"135\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Poisonous</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M234.41,-222.91C229.34,-214.2 223.94,-204.9 218.71,-195.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"221.6,-193.91 213.55,-187.02 215.55,-197.43 221.6,-193.91\"/>\n<text text-anchor=\"middle\" x=\"207.15\" y=\"-207.49\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#6ab6ec\" stroke=\"black\" d=\"M370.5,-187C370.5,-187 283.5,-187 283.5,-187 277.5,-187 271.5,-181 271.5,-175 271.5,-175 271.5,-116 271.5,-116 271.5,-110 277.5,-104 283.5,-104 283.5,-104 370.5,-104 370.5,-104 376.5,-104 382.5,-110 382.5,-116 382.5,-116 382.5,-175 382.5,-175 382.5,-181 376.5,-187 370.5,-187\"/>\n<text text-anchor=\"start\" x=\"279.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Cap Color ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"279.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.722</text>\n<text text-anchor=\"start\" x=\"289.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\n<text text-anchor=\"start\" x=\"287.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1, 4]</text>\n<text text-anchor=\"start\" x=\"284.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Edible</text>\n</g>\n<!-- 0&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>0&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M281.94,-222.91C287.08,-214.2 292.56,-204.9 297.87,-195.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"301.04,-197.41 303.1,-187.02 295.01,-193.86 301.04,-197.41\"/>\n<text text-anchor=\"middle\" x=\"309.35\" y=\"-207.53\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M114,-68C114,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,0 12,0 12,0 114,0 114,0 120,0 126,-6 126,-12 126,-12 126,-56 126,-56 126,-62 120,-68 114,-68\"/>\n<text text-anchor=\"start\" x=\"23\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"25.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\n<text text-anchor=\"start\" x=\"23.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Poisonous</text>\n</g>\n<!-- 1&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M142.71,-103.73C131.81,-94.33 120.24,-84.35 109.38,-74.99\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111.48,-72.18 101.62,-68.3 106.91,-77.48 111.48,-72.18\"/>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#399de5\" stroke=\"black\" d=\"M233.5,-68C233.5,-68 156.5,-68 156.5,-68 150.5,-68 144.5,-62 144.5,-56 144.5,-56 144.5,-12 144.5,-12 144.5,-6 150.5,0 156.5,0 156.5,0 233.5,0 233.5,0 239.5,0 245.5,-6 245.5,-12 245.5,-12 245.5,-56 245.5,-56 245.5,-62 239.5,-68 233.5,-68\"/>\n<text text-anchor=\"start\" x=\"155\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"157.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"155.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1]</text>\n<text text-anchor=\"start\" x=\"152.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Edible</text>\n</g>\n<!-- 1&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>1&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M191.86,-103.73C192.24,-95.52 192.63,-86.86 193.01,-78.56\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"196.52,-78.45 193.48,-68.3 189.53,-78.13 196.52,-78.45\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#399de5\" stroke=\"black\" d=\"M361.5,-68C361.5,-68 284.5,-68 284.5,-68 278.5,-68 272.5,-62 272.5,-56 272.5,-56 272.5,-12 272.5,-12 272.5,-6 278.5,0 284.5,0 284.5,0 361.5,0 361.5,0 367.5,0 373.5,-6 373.5,-12 373.5,-12 373.5,-56 373.5,-56 373.5,-62 367.5,-68 361.5,-68\"/>\n<text text-anchor=\"start\" x=\"283\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"285.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\n<text text-anchor=\"start\" x=\"283.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 4]</text>\n<text text-anchor=\"start\" x=\"280.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Edible</text>\n</g>\n<!-- 4&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M325.51,-103.73C325.21,-95.52 324.89,-86.86 324.59,-78.56\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"328.08,-78.17 324.22,-68.3 321.08,-78.42 328.08,-78.17\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M506,-68C506,-68 404,-68 404,-68 398,-68 392,-62 392,-56 392,-56 392,-12 392,-12 392,-6 398,0 404,0 404,0 506,0 506,0 512,0 518,-6 518,-12 518,-12 518,-56 518,-56 518,-62 512,-68 506,-68\"/>\n<text text-anchor=\"start\" x=\"415\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"417.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"415.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1, 0]</text>\n<text text-anchor=\"start\" x=\"400\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Poisonous</text>\n</g>\n<!-- 4&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>4&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M374.66,-103.73C385.64,-94.33 397.31,-84.35 408.26,-74.99\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"410.75,-77.46 416.07,-68.3 406.2,-72.14 410.75,-77.46\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict a new example\n",
        "\n",
        "The final step is to apply the built tree to predict a new mushroom example."
      ],
      "metadata": {
        "id": "T8o8X9-q13rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_X = np.array([['Brown','Enlarging','Yes'], ['Brown','Enlarging','No']])\n",
        "\n",
        "###### START OF YOUR CODE ######\n",
        "# Hint: Do not forget to convert the categorial feature in the same way as \n",
        "#       when you built the tree.\n",
        "# Save the predictions in `new_y_preds`.\n",
        "new_X_oh = np.zeros_like(new_X, dtype=int)\n",
        "for c in range(new_X.shape[1]):\n",
        "    new_X_oh[:,c] = label_encoders[c].transform(new_X[:,c])\n",
        "new_y_preds = model.predict(new_X_oh)\n",
        "###### END OF YOUR CODE ######\n",
        "\n",
        "for i in range(len(new_X)):\n",
        "    print(f\"{new_X[i]}: {new_y_preds[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH_OlaD8Zg4X",
        "outputId": "2db82521-55df-4ed4-f39c-99fed283f8a0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Brown' 'Enlarging' 'Yes']: 1\n",
            "['Brown' 'Enlarging' 'No']: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected outputs:**\n",
        "```\n",
        "['Brown' 'Enlarging' 'Yes']: 1\n",
        "['Brown' 'Enlarging' 'No']: 0\n",
        "```"
      ],
      "metadata": {
        "id": "ocK_TckA3WwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTgM3kEI281-"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}